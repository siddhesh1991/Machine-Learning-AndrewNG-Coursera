{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from math import exp\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#Importing the Iris dataset.\n",
    "data_df = pd.read_csv('iris.dat',header= None)\n",
    "data_df.columns = ['A', 'B', 'C', 'D', 'E']\n",
    "#Creating the 2 class dataset\n",
    "two_class = data_df[((data_df.E=='Iris-setosa') == True) | ((data_df.E== 'Iris-versicolor') == True)]\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Applying Hot encoding.\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "two_class.head()\n",
    "two_class['E'] = label_encoder.fit_transform(two_class['E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A    B    C    D  E\n",
       "95  5.7  3.0  4.2  1.2  1\n",
       "96  5.7  2.9  4.2  1.3  1\n",
       "97  6.2  2.9  4.3  1.3  1\n",
       "98  5.1  2.5  3.0  1.1  1\n",
       "99  5.7  2.8  4.1  1.3  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_class.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_2class = np.asarray(two_class.iloc[:,:two_class.shape[1]-1])\n",
    "y_2class = np.asarray(two_class['E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for measuring the accuracy.\n",
    "def accuracy(predicted,actual):\n",
    "    tot = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == actual[i]:\n",
    "            tot += 1\n",
    "    acc = round(1. *tot/len(predicted),5)\n",
    "    #print \"Accuracy: \",acc\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function for classification.\n",
    "def classify(param,X):\n",
    "    y_predicted = []\n",
    "    theta_t =  np.transpose(param)\n",
    "    for i in range(len(X)):\n",
    "        x =  np.transpose(X[i]).reshape(len(X[i]),1)\n",
    "        sigmoid =  1.*1/(1+exp(-(np.dot(theta_t,x))))\n",
    "        if sigmoid < 0.5 :\n",
    "            y_predicted.append(0)\n",
    "        else:\n",
    "            y_predicted.append(1)\n",
    "\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_final(y_predicted,y_actual):\n",
    "    final_predicted = []\n",
    "    final_actual = []\n",
    "    for i in range(len(y_predicted)):\n",
    "        for j in range(len(y_predicted[i])):\n",
    "            final_predicted.append(y_predicted[i][j])\n",
    "            final_actual.append(y_actual[i][j])\n",
    "    return(final_predicted,final_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for calculating the error\n",
    "def calculate_error(theta,X,y):\n",
    "    theta_t =  np.transpose(theta)\n",
    "    x =  np.transpose(X).reshape(len(X),1)\n",
    "    theta_tx = np.dot(theta_t,x)\n",
    "    \n",
    "    sigmoid =  1.*1/(1+exp(-(theta_tx)))\n",
    "    return (sigmoid-y)*theta_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function used as a substitute for indicator function.\n",
    "def calculate_relevance(theta,X,y):\n",
    "    theta_t =  np.transpose(theta)\n",
    "    x =  np.transpose(X).reshape(len(X),1)\n",
    "    #print np.dot(theta_t,x)\n",
    "    sigmoid =  1.*1/(1+exp(-(np.dot(theta_t,x))))\n",
    "    #print (sigmoid-y)*X\n",
    "    return ((sigmoid-y)*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def gradient_descent(X,y,threshold=0.0001,learn_rt=0.001): \n",
    "        #Setting the initial convergence condition as False\n",
    "        converge = False\n",
    "        theta = np.zeros(shape=(X.shape[1],1))\n",
    "        iterations = 0\n",
    "        #print theta\n",
    "        while converge ==False:\n",
    "            #print \"s\"\n",
    "            theta0 = theta.copy()\n",
    "            #Calculating the Gradient\n",
    "            grad = []\n",
    "            for i in range(len(X)):\n",
    "                grad.append(calculate_relevance(theta0,X[i],y[i]))\n",
    "            gradient = [list(i) for i in zip(*grad)]\n",
    "            summ = [sum(i) for i in gradient]\n",
    "            theta =  np.transpose((np.transpose(theta0)[0]) - np.dot(learn_rt,summ))\n",
    "            \n",
    "            err_theta0 = []\n",
    "            err_theta = []\n",
    "            for i in range(len(X)):\n",
    "                err_theta0.append(calculate_error(theta0,X[i],y[i]))\n",
    "            for i in range(len(X)):\n",
    "                err_theta.append(calculate_error(theta,X[i],y[i]))\n",
    "            iterations += 1\n",
    "            if (iterations == 200 or abs(sum(err_theta)-sum(err_theta0)) < threshold):\n",
    "                converge = True\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression 2-class classification is 97.00 percent.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#Doing cross validation for 2 class logistic regression\n",
    "def cross_validation(X, y, n_folds=10,poly = 1):\n",
    "        poly_degree = PolynomialFeatures(poly)\n",
    "        cv = KFold(len(y), n_folds,shuffle = True)\n",
    "        fold_no = 0\n",
    "        kfold_accuracy = []\n",
    "        train_error = []\n",
    "        test_error = []\n",
    "        results = []\n",
    "        actual = []\n",
    "        for train_ind, test_ind in cv:\n",
    "            #Calling module for parameter calculation(X_train,y_train)\n",
    "            #Calling module for classification(X_test,y_test)\n",
    "            Z_train = poly_degree.fit_transform(X[train_ind])\n",
    "            Z_test = poly_degree.fit_transform(X[test_ind])\n",
    "            param = gradient_descent(Z_train,y[train_ind])\n",
    "            \n",
    "            results.append(classify(param,Z_test))\n",
    "            actual.append(y[test_ind])\n",
    "        final = create_final(results,actual)\n",
    "        print \"Accuracy of the Logistic Regression 2-class classification is %.2f percent.\" %(accuracy(final[0],final[1])*100)\n",
    "        return (final)\n",
    "Cross_val = cross_validation(X_2class,y_2class)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression for 3-class classification.\n",
    "data_df['E'] = label_encoder.fit_transform(data_df['E'])\n",
    "X_3class = np.asarray(data_df.iloc[:,:data_df.shape[1]-1])\n",
    "y_3class = np.asarray(data_df['E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Indicator function fro 3 class Logistic regression.\n",
    "def calculate_relevance_3class(theta_vec,theta,X,y,cls,classes,typ = 'likelihood'):\n",
    "    \n",
    "    x =  np.transpose(X).reshape(len(X),1)\n",
    "    num = exp(np.dot(theta,x))\n",
    "    tot = 0\n",
    "    for i in range(len(classes)):\n",
    "        tot += exp(np.dot(theta_vec[i],x))\n",
    "    if num == 0:\n",
    "        h_theta = 0\n",
    "        \n",
    "    else:\n",
    "        h_theta = 1.*num/tot\n",
    "    grad = (h_theta - 1) * X\n",
    "    if typ!= 'likelihood':\n",
    "        return grad\n",
    "    else:\n",
    "        return h_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Doing classification for 3 class logistic regression.\n",
    "def do_classification(X,theta,classes):\n",
    "    y_predicted = []\n",
    "    for i in range(len(X)):\n",
    "        y_hat = []\n",
    "        for j in range(len(classes)):\n",
    "            t= theta[j].reshape(1,len(theta[j]))\n",
    "            x= X[i].reshape(len(X[j]),1)\n",
    "            y_hat.append(np.dot(t,x))\n",
    "        index = y_hat.index(max(y_hat))\n",
    "        y_predicted.append(index)\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for calculating Log Likelihood.\n",
    "def calc_log_likelihood(T,X,y,classes):\n",
    "    #Calculating Log Likelihood.  \n",
    "    likelihood = 0\n",
    "    for cls in range(len(classes)):\n",
    "        for j in range(len(X)):\n",
    "            if y[j] == cls:\n",
    "                likelihood += log(calculate_relevance_3class(T,T[cls],X[j],y[j],cls,classes,typ = 'likelihood'))\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient descent for 3 class logistic regression\n",
    "def gradient_descent_3class(classes,X,y,threshold=0.01,learn_rt=0.00001): \n",
    "        #Setting the initial convergence condition as False\n",
    "        converge = False\n",
    "        iterations = 0\n",
    "        theta_vec = np.zeros(shape=(len(classes),X.shape[1]))\n",
    "        while converge == False:\n",
    "            t0 = theta_vec.copy()\n",
    "        #for cls in range(len(classes)):\n",
    "            #while converge == False:\n",
    "            for cls in range(len(classes)):\n",
    "                theta0 = theta_vec[cls]\n",
    "                #Calculating the Gradient\n",
    "                grad = []\n",
    "                for i in range(len(X)):\n",
    "                    if y[i] == cls:\n",
    "                        grad.append(calculate_relevance_3class(theta_vec,theta_vec[cls],X[i],y[i],cls,classes,typ='gradient'))\n",
    "                    else:\n",
    "                        pass\n",
    "                gradient = [list(i) for i in zip(*grad)]\n",
    "                \n",
    "                summ = [sum(i) for i in gradient]\n",
    "                \n",
    "                #theta_vec[cls] =  theta0 - np.dot(learn_rt,summ)\n",
    "                subtr = np.dot(learn_rt,summ)\n",
    "                theta_vec[cls] = [theta0[i] - subtr[i] for i in range(len(subtr))]\n",
    "            t1 = theta_vec\n",
    "            #Calculating Difference of the Previous and the Current Log Likelihood.\n",
    "            delta = calc_log_likelihood(t1,X,y,classes) - calc_log_likelihood(t0,X,y,classes)\n",
    "            iterations += 1\n",
    "            if (iterations == 185 ):\n",
    "                converge = True\n",
    "       \n",
    "        return theta_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression 3-class classification is 77.33 percent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:6: FutureWarning: the 'cols' keyword is deprecated, use 'subset' instead\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def cross_validation_3class(X, y, n_folds=10,poly = 2):\n",
    "        poly_degree = PolynomialFeatures(poly)\n",
    "        cv = KFold(len(y), n_folds,shuffle = True)\n",
    "        fold_no = 0\n",
    "        data_df.drop_duplicates(cols = 'E', inplace = True)\n",
    "        classes = data_df['E'].tolist()\n",
    "        kfold_accuracy = []\n",
    "        train_error = []\n",
    "        test_error = []\n",
    "        results = []\n",
    "        actual = []\n",
    "        for train_ind, test_ind in cv:\n",
    "            #Calling module for parameter calculation(X_train,y_train)\n",
    "            #Calling module for classification(X_test,y_test)\n",
    "            Z_train = poly_degree.fit_transform(X[train_ind])\n",
    "            Z_test = poly_degree.fit_transform(X[test_ind])\n",
    "            param = gradient_descent_3class(classes,Z_train,y[train_ind])\n",
    "            results.append(do_classification(Z_test,param,classes))\n",
    "            actual.append(y[test_ind])\n",
    "        final = create_final(results,actual)\n",
    "        print \"Accuracy of the Logistic Regression 3-class classification is %.2f percent.\" %(accuracy(final[0],final[1])*100)\n",
    "Call_cross_validation = cross_validation_3class(X_3class,y_3class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  #========Neural Networks=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18623, 784) 18623\n"
     ]
    }
   ],
   "source": [
    "#For 3 class Neural Networks for 2 layer feedforward Multi Layer Perceptron.\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "import os\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from pylab import *\n",
    "%matplotlib inline\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "#print mnist.data.shape\n",
    "#print mnist.target.shape\n",
    "np.unique(mnist.target)\n",
    "\n",
    "X, y = mnist.data / 255., mnist.target\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "size=len(y_train)\n",
    "\n",
    "## extract digits '0' and '1' for 2 class Logistic regression.\"\n",
    "ind_2class = [ k for k in range(size) if y_train[k]==0 or y_train[k] == 1 or y_train[k] == 2]\n",
    "extracted_images_2class=X_train[ind_2class,:]\n",
    "y_actual_3class = y_train[ind_2class]\n",
    "#print extracted_images_2class.shape\n",
    "\n",
    "X_3class = np.asarray(extracted_images_2class)\n",
    "print (X_3class).shape,len(y_actual_3class)\n",
    "#print X_2class\n",
    "#mean_image=extracted_images.mean(axis=0)\n",
    "#imshow(mean_image.reshape(28,28), cmap=cm.gray)\n",
    "#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Selecting random records for further analysis\n",
    "idx = np.random.choice(np.arange(len(X_3class)),500, replace=False)\n",
    "X_3class = X_3class[idx]\n",
    "y_actual_3class = y_actual_3class[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculating Z\n",
    "def calculate_zi(W,X):\n",
    "    Z_i = []\n",
    "    \n",
    "    x = X.reshape(len(X),1)\n",
    "   \n",
    "    for i in range(len(W)):\n",
    "        w = W[i].reshape(1,len(W[i]))\n",
    "        Z_i.append((1.*1/(1+exp(-(np.dot(w[0],x))))).tolist()[0])\n",
    "        \n",
    "    return Z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating the predicted value using softmax function\n",
    "def calculate_y_ji_hat(V,Vj,Zi):\n",
    "    y_hat = []\n",
    "    den = 0\n",
    "    Vj = Vj.reshape(1,len(Vj))\n",
    "    #Calculating denominator of softmax function.\n",
    "    for i in range(len(V)):\n",
    "        vc = V[i].reshape(1,len(V[i]))\n",
    "        den += exp(np.dot(vc,Zi))\n",
    "    return 1.*exp(np.dot(Vj,Zi))/den #Returning the softmax function value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_params_nn(X,y,classes,poly_degree,h=200,lr = 0.00001,threshold = 0):\n",
    "    #Initial guess of Vj and Wj = 0.\n",
    "    #V Dimensions --> Num of classes * number of hidden units.\n",
    "    V_vec = np.zeros(shape=(len(classes),h+1))\n",
    "    #W Dimensions --> Num of Hidde Units * Number of features.\n",
    "    W_vec = np.zeros(shape = (h,len(X[0])))   \n",
    "    iterations = 0\n",
    "    converge = False\n",
    "    while converge == False:\n",
    "        V_vec_prev = V_vec.copy()\n",
    "        W_vec_prev = W_vec.copy()\n",
    "        #Calclulation of Zi and yi hat.\n",
    "        #z_i = []\n",
    "        #y_ihat = []\n",
    "        #Calculating Zi and y_ji_hat.\n",
    "        #print poly_degree\n",
    "        for cls in range(len(classes)):\n",
    "            grad = 0\n",
    "            for i in range(len(X)):\n",
    "                if y[i] == cls:\n",
    "                    z_arr = calculate_zi(W_vec_prev,X[i])\n",
    "                    z_i = poly_degree.fit_transform(z_arr)[0]\n",
    "                    y_ihat = (calculate_y_ji_hat(V_vec_prev,V_vec_prev[cls],z_i)) \n",
    "                    grad += (y_ihat - 1)*z_i\n",
    "            #Update equation for V.\n",
    "            V_vec[cls] =  V_vec_prev[cls] - lr*grad\n",
    "            \n",
    "        #Updating W.\n",
    "        for i in range(len(W_vec_prev)):\n",
    "            final_grad = 0\n",
    "            for j in range(len(X)):\n",
    "                grad = 0\n",
    "                z_arr = calculate_zi(W_vec_prev,X[j])\n",
    "                z_i = poly_degree.fit_transform(z_arr)[0]\n",
    "                for cls in range(len(classes)):\n",
    "                    y_ihat = (calculate_y_ji_hat(V_vec,V_vec[cls],z_i))\n",
    "                    grad += (y_ihat - y[j])*V_vec[cls][i+1]\n",
    "                final_grad += grad*z_arr[i]*(1-z_arr[i])*X[j]\n",
    "            W_vec[i] = W_vec_prev[i] - lr*final_grad\n",
    "        iterations += 1\n",
    "        print iterations\n",
    "        if (iterations == 50):\n",
    "            converge = True\n",
    "            return V_vec,W_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_classification(V,W,X,classes,poly_degree,h=10):\n",
    "    results = []\n",
    "    for i in range(len(X)):\n",
    "        y_ihat = []\n",
    "        for j in range(len(classes)):\n",
    "                    z_arr = calculate_zi(W,X[i])\n",
    "                    z_i = poly_degree.fit_transform(z_arr)[0]\n",
    "                    y_ihat.append(calculate_y_ji_hat(V,V[j],z_i)) \n",
    "        results.append(y_ihat.index(max(y_ihat)))\n",
    "    return results\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Neural-Networks 3-class classification is 0.97.\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_nn(X, y,n_folds=10,poly = 1):\n",
    "        poly_degree = PolynomialFeatures(poly)\n",
    "        cv = KFold(len(y), n_folds,shuffle = True)\n",
    "        fold_no = 0\n",
    "        kfold_accuracy = []\n",
    "        train_error = []\n",
    "        test_error = []\n",
    "        classes = list(set(y))\n",
    "        for train_ind, test_ind in cv:\n",
    "            #Calling module for parameter calculation(X_train,y_train)\n",
    "            #Calling module for classification(X_test,y_test)\n",
    "            Z_train = poly_degree.fit_transform(X[train_ind])\n",
    "            Z_test = poly_degree.fit_transform(X[test_ind])\n",
    "            params = calc_params_nn(Z_train,y[train_ind],classes,poly_degree,h=10,threshold = 0)\n",
    "            #print y[test_ind]\n",
    "            results = do_classification(params[0],params[1],Z_test,classes,poly_degree)\n",
    "            #print results\n",
    "            actual = y[test_ind].tolist()\n",
    "            print \"Accuracy of Neural-Networks 3-class classification is %.2f.\" %accuracy(Cross_val[0],Cross_val[1])   \n",
    "            break\n",
    "cross_validation_nn(X_3class,y_actual_3class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
